{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "def convert_folder_images(folder_name):\n",
    "\n",
    "    # specify the folder path\n",
    "    folder_path = f\"alphabet_data/{folder_name}\"\n",
    "\n",
    "    # initialize an empty list to store the image arrays\n",
    "    image_arrays = []\n",
    "\n",
    "    # loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # check that the file is an image file (e.g. JPEG, PNG)\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            # construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # read the image file as a numpy array\n",
    "            img_array = cv2.imread(file_path)\n",
    "\n",
    "            #gray the images\n",
    "            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY) # from 3 channels to 1 channel\n",
    "            # coverting to gray scale gets rid of one dimension, so we add it back in, this time with 1 channel\n",
    "            # Add an extra dimension to the grayscale image\n",
    "            img_array = np.expand_dims(img_array, axis=-1)\n",
    "\n",
    "            # add the image array to the list\n",
    "            image_arrays.append(img_array)\n",
    "\n",
    "    # convert the list of image arrays to a numpy array\n",
    "    image_arrays = np.array(image_arrays)\n",
    "    return image_arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "def get_all_images_and_labels():\n",
    "    a = convert_folder_images(\"a\")\n",
    "    # make labels for all photos based on how many images are in the folder\n",
    "    # the folder names gives you the label for each folder of images\n",
    "\n",
    "    b = convert_folder_images(\"b\")\n",
    "    c = convert_folder_images(\"c\")\n",
    "    d = convert_folder_images(\"d\")\n",
    "    e = convert_folder_images(\"e\")\n",
    "\n",
    "    labels_a = np.full(len(a),\"a\")\n",
    "    labels_b = np.full(len(b),\"b\")\n",
    "    labels_c = np.full(len(c),\"c\")\n",
    "    labels_d = np.full(len(d),\"d\")\n",
    "    labels_e = np.full(len(e),\"e\")\n",
    "    # print(labels_e.shape)\n",
    "    images = np.vstack((a,b,c,d,e))/255 # divide by 255 for normalization\n",
    "    # labels = np.append(labels_a, (labels_b, labels_c, labels_d, labels_e))\n",
    "\n",
    "    # Convert string labels to numerical labels using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    # concatenate all the labels together\n",
    "    labels = np.concatenate(\n",
    "        (labels_a, labels_b, labels_c, labels_d, labels_e), axis=0)\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "    return images,labels\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "images,labels = get_all_images_and_labels()\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=2)\n",
    "# img_index = 60\n",
    "# x_train.shape\n",
    "# plt.imshow(x_train[img_index])\n",
    "# normalize data dimensions\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# make training, validation, and testing sets\n",
    "\n",
    "# split the train data into train and validation sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=2)\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "\n",
    "# use Sequential model API\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(300, 300,1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(300, 300,1))\n",
    "        self.pool1 = MaxPooling2D(pool_size=2)\n",
    "        self.dropout1 = Dropout(0.3)\n",
    "        self.conv2 = Conv2D(filters=32, kernel_size=2, padding='same', activation='relu')\n",
    "        self.pool2 = MaxPooling2D(pool_size=2)\n",
    "        self.dropout2 = Dropout(0.3)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(256, activation='relu')\n",
    "        self.dropout3 = Dropout(0.5)\n",
    "        self.dense2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense2(x)\n",
    "        return x#%%\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def convert_folder_images(folder_name):\n",
    "\n",
    "    # specify the folder path\n",
    "    folder_path = f\"alphabet_data/{folder_name}\"\n",
    "\n",
    "    # initialize an empty list to store the image arrays\n",
    "    image_arrays = []\n",
    "\n",
    "    # loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # check that the file is an image file (e.g. JPEG, PNG)\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            # construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # read the image file as a numpy array\n",
    "            img_array = cv2.imread(file_path)\n",
    "\n",
    "            #gray the images\n",
    "            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY) # from 3 channels to 1 channel\n",
    "            # coverting to gray scale gets rid of one dimension, so we add it back in, this time with 1 channel\n",
    "            # Add an extra dimension to the grayscale image\n",
    "            img_array = np.expand_dims(img_array, axis=-1)\n",
    "\n",
    "            # add the image array to the list\n",
    "            image_arrays.append(img_array)\n",
    "\n",
    "    # convert the list of image arrays to a numpy array\n",
    "    image_arrays = np.array(image_arrays)\n",
    "    return image_arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def get_all_images_and_labels():\n",
    "    a = convert_folder_images(\"a\")\n",
    "    # make labels for all photos based on how many images are in the folder\n",
    "    # the folder names gives you the label for each folder of images\n",
    "\n",
    "    b = convert_folder_images(\"b\")\n",
    "    c = convert_folder_images(\"c\")\n",
    "    d = convert_folder_images(\"d\")\n",
    "    e = convert_folder_images(\"e\")\n",
    "\n",
    "    labels_a = np.full(len(a),\"a\")\n",
    "    labels_b = np.full(len(b),\"b\")\n",
    "    labels_c = np.full(len(c),\"c\")\n",
    "    labels_d = np.full(len(d),\"d\")\n",
    "    labels_e = np.full(len(e),\"e\")\n",
    "    # print(labels_e.shape)\n",
    "    images = np.vstack((a,b,c,d,e))/255 # divide by 255 for normalization\n",
    "    # labels = np.append(labels_a, (labels_b, labels_c, labels_d, labels_e))\n",
    "\n",
    "    # Convert string labels to numerical labels using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    # concatenate all the labels together\n",
    "    labels = np.concatenate(\n",
    "        (labels_a, labels_b, labels_c, labels_d, labels_e), axis=0)\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "    return images,labels\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "(250, 300, 300, 1)"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images,labels = get_all_images_and_labels()\n",
    "images.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "(250,)"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "(300, 300, 1)"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_image = images[5]\n",
    "one_image[:][:][:].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "# plt.imshow(one_image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=2)\n",
    "# img_index = 60\n",
    "# x_train.shape\n",
    "# plt.imshow(x_train[img_index])\n",
    "# normalize data dimensions\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# make training, validation, and testing sets\n",
    "\n",
    "# split the train data into train and validation sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=2)\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(300, 300,1))\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.3)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dropout3 = tf.keras.layers.Dropout(0.5)\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    # @tf.autograph.experimental.do_not_convert\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "data": {
      "text/plain": "(None,)"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "# configure the learning process\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              # use sparse cross entropy since your data is integer encoded, NOT hot-encoded (0s and 1s)\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']),\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead\")\n",
    "    device_name = '/cpu:0'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "(None,)"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# configure the learning process\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              # use sparse cross entropy since your data is integer encoded, NOT hot-encoded (0s and 1s)\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']),\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def fit_model():\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "              batch_size=50,\n",
    "              epochs=10,\n",
    "              validation_data=(x_valid, y_valid), verbose=1,\n",
    "              callbacks=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025434D68EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000025434D68EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MyModel.call of <__main__.MyModel object at 0x0000025434D57548>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MyModel.call of <__main__.MyModel object at 0x0000025434D57548>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4/4 [==============================] - ETA: 0s - loss: 8.5464 - accuracy: 0.2188WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025430FB73A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000025430FB73A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4/4 [==============================] - 11s 2s/step - loss: 8.5464 - accuracy: 0.2188 - val_loss: 2.2294 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 9s 2s/step - loss: 4.1877 - accuracy: 0.6313 - val_loss: 0.6601 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.9094 - accuracy: 0.6500 - val_loss: 1.0975 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.4524 - accuracy: 0.9187 - val_loss: 1.8649 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.3895 - accuracy: 0.9375 - val_loss: 1.5618 - val_accuracy: 0.9500\n",
      "Epoch 6/10\n",
      "2/4 [==============>...............] - ETA: 4s - loss: 0.2439 - accuracy: 0.9600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12040\\2723512273.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;31m# train the model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mfit_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[1;31m# @tf.autograph.experimental.do_not_convert\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;31m# model.fit(x_train,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    640\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    641\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mControlStatusCtx\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstatus\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mStatus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDISABLED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 642\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    643\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    644\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mismethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12040\\3486007071.py\u001B[0m in \u001B[0;36mfit_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m               \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m               \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_valid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_valid\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m               callbacks=None)\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\envs\\apply_ai\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\apply_ai\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1648\u001B[0m                         ):\n\u001B[0;32m   1649\u001B[0m                             \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1650\u001B[1;33m                             \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1651\u001B[0m                             \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1652\u001B[0m                                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    878\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    879\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 880\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    881\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    882\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    910\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    911\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 912\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_no_variable_creation_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    913\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_variable_creation_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    914\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    133\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m    134\u001B[0m     return concrete_function._call_flat(\n\u001B[1;32m--> 135\u001B[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    136\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1744\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1745\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1746\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1747\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1748\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    381\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    382\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 383\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    384\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    385\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 53\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     54\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a TensorFlow session and set it to use the specified device\n",
    "\n",
    "with tf.device(device_name):\n",
    "    # Your TensorFlow code here\n",
    "\n",
    "    # train the model\n",
    "    fit_model()\n",
    "    # @tf.autograph.experimental.do_not_convert\n",
    "    # model.fit(x_train,\n",
    "    #           y_train,\n",
    "    #           batch_size=50,\n",
    "    #           epochs=10,\n",
    "    #           validation_data=(x_valid, y_valid), verbose=1,\n",
    "    #           callbacks=None)\n",
    " # the number on the left below Epochs is number batch you are on.\n",
    "# @tf.autograph.experimental.do_not_convert\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])\n",
    "# model.predict(x_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
